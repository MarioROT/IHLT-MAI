{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarioROT/IHLT-MAI/blob/main/Session7_MarioRosas_AlamLopez.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpEp-jhafvsx"
      },
      "source": [
        "# Lab session 7 (Word Sequences) - IHLT\n",
        "\n",
        "**Students:**\n",
        "- Mario Rosas\n",
        "- Alam Lopez\n",
        "\n",
        "**Lab Professor:** Salvador Medina Herrera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0LXucmalIYk"
      },
      "source": [
        "## Paraphrases Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeQ11CQB4e3o",
        "outputId": "9d783fa6-73d9-4d10-dfd1-cc9b58261f8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'IHLT-MAI'...\n",
            "remote: Enumerating objects: 387, done.\u001b[K\n",
            "remote: Counting objects: 100% (197/197), done.\u001b[K\n",
            "remote: Compressing objects: 100% (144/144), done.\u001b[K\n",
            "remote: Total 387 (delta 135), reused 89 (delta 53), pack-reused 190\u001b[K\n",
            "Receiving objects: 100% (387/387), 324.75 KiB | 3.18 MiB/s, done.\n",
            "Resolving deltas: 100% (225/225), done.\n",
            "Collecting svgling\n",
            "  Downloading svgling-0.4.0-py3-none-any.whl (23 kB)\n",
            "Collecting svgwrite (from svgling)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.4.0 svgwrite-1.4.3\n",
            "Collecting python-crfsuite\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.9\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "git clone https://github.com/mariorot/IHLT-MAI.git\n",
        "cd 'IHLT-MAI'\n",
        "mv 'complementary_material' /content/\n",
        "mv scripts /content/\n",
        "pip install svgling\n",
        "pip install python-crfsuite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6xL1WCD-EKut"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mi9UgnrHx1NN",
        "outputId": "15d86f7f-008f-4677-cd64-7fadb4ac3baa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet_ic to /home/mario-\n",
            "[nltk_data]     rot/nltk_data...\n",
            "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /home/mario-\n",
            "[nltk_data]     rot/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/mario-rot/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/mario-rot/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package treebank to /home/mario-\n",
            "[nltk_data]     rot/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/mario-\n",
            "[nltk_data]     rot/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /home/mario-\n",
            "[nltk_data]     rot/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet_ic to /home/mario-\n",
            "[nltk_data]     rot/nltk_data...\n",
            "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to /home/mario-\n",
            "[nltk_data]     rot/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /home/mario-rot/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from scripts.compute_metrics import ComputeMetrics\n",
        "from scripts.text_preprocessing import TextPreprocessing\n",
        "from scripts.utils import Dataset, ShowResults\n",
        "import pandas as pd\n",
        "pd.set_option(\"display.precision\", 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = Dataset('complementary_material/train/')\n",
        "test = Dataset('complementary_material/test-gold/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HHcaJhckB_Q"
      },
      "source": [
        "## Experiments on training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4rtq6CjC_Kqp"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>jaccard</th>\n",
              "      <th>cosine</th>\n",
              "      <th>overlap</th>\n",
              "      <th>dice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NLTK TOKEN</td>\n",
              "      <td>0.450</td>\n",
              "      <td>0.461</td>\n",
              "      <td>0.444</td>\n",
              "      <td>0.462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SPACY TOKEN</td>\n",
              "      <td>0.461</td>\n",
              "      <td>0.472</td>\n",
              "      <td>0.458</td>\n",
              "      <td>0.473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CLEAN TOKEN</td>\n",
              "      <td>0.468</td>\n",
              "      <td>0.473</td>\n",
              "      <td>0.443</td>\n",
              "      <td>0.475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NLTK LEMMA</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.462</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SPACY LEMMA</td>\n",
              "      <td>0.477</td>\n",
              "      <td>0.492</td>\n",
              "      <td>0.470</td>\n",
              "      <td>0.494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CLEAN NLTK LEMMA</td>\n",
              "      <td>0.481</td>\n",
              "      <td>0.496</td>\n",
              "      <td>0.465</td>\n",
              "      <td>0.498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CLEAN SPACY LEMMA</td>\n",
              "      <td>0.491</td>\n",
              "      <td>0.505</td>\n",
              "      <td>0.476</td>\n",
              "      <td>0.507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NLTK TOKEN WSD</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.417</td>\n",
              "      <td>0.398</td>\n",
              "      <td>0.419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SPACY TOKEN WSD</td>\n",
              "      <td>0.429</td>\n",
              "      <td>0.425</td>\n",
              "      <td>0.408</td>\n",
              "      <td>0.426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CLEAN TOKEN WSD</td>\n",
              "      <td>0.480</td>\n",
              "      <td>0.479</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NLTK LEMMA WSD</td>\n",
              "      <td>0.417</td>\n",
              "      <td>0.408</td>\n",
              "      <td>0.388</td>\n",
              "      <td>0.410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SPACY LEMMA WSD</td>\n",
              "      <td>0.422</td>\n",
              "      <td>0.409</td>\n",
              "      <td>0.388</td>\n",
              "      <td>0.411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>CLEAN NLTK LEMMA WSD</td>\n",
              "      <td>0.482</td>\n",
              "      <td>0.483</td>\n",
              "      <td>0.455</td>\n",
              "      <td>0.485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>CLEAN SPACY LEMMA WSD</td>\n",
              "      <td>0.493</td>\n",
              "      <td>0.492</td>\n",
              "      <td>0.466</td>\n",
              "      <td>0.494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NLTK NE</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.445</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SPACY NE</td>\n",
              "      <td>0.427</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.427</td>\n",
              "      <td>0.431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CLEAN NLTK NE</td>\n",
              "      <td>0.468</td>\n",
              "      <td>0.473</td>\n",
              "      <td>0.443</td>\n",
              "      <td>0.475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CLEAN SPACY NE</td>\n",
              "      <td>0.456</td>\n",
              "      <td>0.458</td>\n",
              "      <td>0.422</td>\n",
              "      <td>0.460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NLTK LEMMAS - NLTK NE</td>\n",
              "      <td>0.438</td>\n",
              "      <td>0.444</td>\n",
              "      <td>0.424</td>\n",
              "      <td>0.445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NLTK LEMMAS - SPACY NE</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.426</td>\n",
              "      <td>0.418</td>\n",
              "      <td>0.426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>SPACY LEMMAS - NLTK NE</td>\n",
              "      <td>0.468</td>\n",
              "      <td>0.484</td>\n",
              "      <td>0.466</td>\n",
              "      <td>0.485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>SPACY LEMMAS - SPACY NE</td>\n",
              "      <td>0.441</td>\n",
              "      <td>0.446</td>\n",
              "      <td>0.435</td>\n",
              "      <td>0.446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>CLEAN NLTK LEMMAS - NLTK NE</td>\n",
              "      <td>0.481</td>\n",
              "      <td>0.496</td>\n",
              "      <td>0.465</td>\n",
              "      <td>0.498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>CLEAN NLTK LEMMAS - SPACY NE</td>\n",
              "      <td>0.468</td>\n",
              "      <td>0.477</td>\n",
              "      <td>0.442</td>\n",
              "      <td>0.480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>CLEAN SPACY LEMMAS - NLTK NE</td>\n",
              "      <td>0.491</td>\n",
              "      <td>0.505</td>\n",
              "      <td>0.476</td>\n",
              "      <td>0.507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>CLEAN SPACY LEMMAS - SPACY NE</td>\n",
              "      <td>0.476</td>\n",
              "      <td>0.485</td>\n",
              "      <td>0.451</td>\n",
              "      <td>0.487</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Category  jaccard  cosine  overlap   dice\n",
              "0                      NLTK TOKEN    0.450   0.461    0.444  0.462\n",
              "1                     SPACY TOKEN    0.461   0.472    0.458  0.473\n",
              "2                     CLEAN TOKEN    0.468   0.473    0.443  0.475\n",
              "0                      NLTK LEMMA    0.449   0.462    0.440  0.464\n",
              "1                     SPACY LEMMA    0.477   0.492    0.470  0.494\n",
              "2                CLEAN NLTK LEMMA    0.481   0.496    0.465  0.498\n",
              "3               CLEAN SPACY LEMMA    0.491   0.505    0.476  0.507\n",
              "0                  NLTK TOKEN WSD    0.420   0.417    0.398  0.419\n",
              "1                 SPACY TOKEN WSD    0.429   0.425    0.408  0.426\n",
              "2                 CLEAN TOKEN WSD    0.480   0.479    0.449  0.481\n",
              "3                  NLTK LEMMA WSD    0.417   0.408    0.388  0.410\n",
              "4                 SPACY LEMMA WSD    0.422   0.409    0.388  0.411\n",
              "5            CLEAN NLTK LEMMA WSD    0.482   0.483    0.455  0.485\n",
              "6           CLEAN SPACY LEMMA WSD    0.493   0.492    0.466  0.494\n",
              "0                         NLTK NE    0.440   0.445    0.430  0.446\n",
              "1                        SPACY NE    0.427   0.431    0.427  0.431\n",
              "2                   CLEAN NLTK NE    0.468   0.473    0.443  0.475\n",
              "3                  CLEAN SPACY NE    0.456   0.458    0.422  0.460\n",
              "4           NLTK LEMMAS - NLTK NE    0.438   0.444    0.424  0.445\n",
              "5          NLTK LEMMAS - SPACY NE    0.420   0.426    0.418  0.426\n",
              "6          SPACY LEMMAS - NLTK NE    0.468   0.484    0.466  0.485\n",
              "7         SPACY LEMMAS - SPACY NE    0.441   0.446    0.435  0.446\n",
              "8     CLEAN NLTK LEMMAS - NLTK NE    0.481   0.496    0.465  0.498\n",
              "9    CLEAN NLTK LEMMAS - SPACY NE    0.468   0.477    0.442  0.480\n",
              "10   CLEAN SPACY LEMMAS - NLTK NE    0.491   0.505    0.476  0.507\n",
              "11  CLEAN SPACY LEMMAS - SPACY NE    0.476   0.485    0.451  0.487"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tp = TextPreprocessing()\n",
        "\n",
        "dt = test['SMTeuroparl']\n",
        "\n",
        "# ----- Tokenization -----\n",
        "# NLTK\n",
        "dt[2] = tp.tokenize_data(list(dt[0]),'nltk')\n",
        "dt[3] = tp.tokenize_data(list(dt[1]),'nltk')\n",
        "# spaCy\n",
        "dt[4] = tp.tokenize_data(list(dt[0]),'spacy')\n",
        "dt[5] = tp.tokenize_data(list(dt[1]),'spacy')\n",
        "# Data cleaning\n",
        "dt[6]=tp.clean_data(list(dt[0]))\n",
        "dt[7]=tp.clean_data(list(dt[1]))\n",
        "\n",
        "\n",
        "# ----- Lemmatization -----\n",
        "# -- With Tokens\n",
        "# NLTK\n",
        "dt[8]=tp.lemmatize_data(list(dt[0]),'nltk',False)\n",
        "dt[9]=tp.lemmatize_data(list(dt[1]),'nltk',False)\n",
        "# spaCy\n",
        "dt[10]=tp.lemmatize_data(list(dt[0]),'spacy')\n",
        "dt[11]=tp.lemmatize_data(list(dt[1]),'spacy')\n",
        "\n",
        "# -- With Cleaned data\n",
        "# NLTK\n",
        "dt[12]=tp.lemmatize_data(list(dt[6]),'nltk')\n",
        "dt[13]=tp.lemmatize_data(list(dt[7]),'nltk')\n",
        "# spaCy\n",
        "dt[14]=tp.lemmatize_data(list(dt[6]),'spacy')\n",
        "dt[15]=tp.lemmatize_data(list(dt[7]),'spacy')\n",
        "\n",
        "\n",
        "# ----- Word Desambiguation -----\n",
        "# --- With Tokens\n",
        "# NLTK\n",
        "dt[16]= tp.wsd_lesk_data(list(dt[2]),'nltk', keep_failures=True, synset_word=True)\n",
        "dt[17]= tp.wsd_lesk_data(list(dt[3]),'nltk', keep_failures=True, synset_word=True)\n",
        "# Spacy\n",
        "dt[18]= tp.wsd_lesk_data(list(dt[4]),'nltk', keep_failures=True, synset_word=True)\n",
        "dt[19]= tp.wsd_lesk_data(list(dt[5]),'nltk', keep_failures=True, synset_word=True)\n",
        "# Cleaned data\n",
        "dt[20]= tp.wsd_lesk_data(list(dt[6]),'nltk', keep_failures=True, synset_word=True)\n",
        "dt[21]= tp.wsd_lesk_data(list(dt[7]),'nltk', keep_failures=True, synset_word=True)\n",
        "\n",
        "# --- With Lemmas\n",
        "# NLTK Lemmas\n",
        "dt[22]= tp.wsd_lesk_data(list(dt[8]),'nltk', keep_failures=True, synset_word=True)\n",
        "dt[23]= tp.wsd_lesk_data(list(dt[9]),'nltk', keep_failures=True, synset_word=True)\n",
        "# Spacy Lemmas\n",
        "dt[24]= tp.wsd_lesk_data(list(dt[10]),'nltk', keep_failures=True, synset_word=True)\n",
        "dt[25]= tp.wsd_lesk_data(list(dt[11]),'nltk', keep_failures=True, synset_word=True)\n",
        "# Cleaned NLTK Lemmas\n",
        "dt[26]= tp.wsd_lesk_data(list(dt[12]),'nltk', keep_failures=True, synset_word=True)\n",
        "dt[27]= tp.wsd_lesk_data(list(dt[13]),'nltk', keep_failures=True, synset_word=True)\n",
        "# Cleaned SpaCy Lemmas\n",
        "dt[28]= tp.wsd_lesk_data(list(dt[14]),'nltk', keep_failures=True, synset_word=True)\n",
        "dt[29]= tp.wsd_lesk_data(list(dt[15]),'nltk', keep_failures=True, synset_word=True)\n",
        "\n",
        "\n",
        "# ----- Named Entities -----\n",
        "# --- With Tokens\n",
        "# NLTK\n",
        "dt[30]= tp.named_entities_data(list(dt[0]), 'nltk', False)\n",
        "dt[31]= tp.named_entities_data(list(dt[1]), 'nltk', False)\n",
        "# SpaCy\n",
        "dt[32]= tp.named_entities_data(list(dt[0]), 'spacy')\n",
        "dt[33]= tp.named_entities_data(list(dt[1]), 'spacy')\n",
        "# Cleaned data\n",
        "# NLTK\n",
        "dt[34]= tp.named_entities_data(list(dt[6]), 'nltk')\n",
        "dt[35]= tp.named_entities_data(list(dt[7]), 'nltk')\n",
        "# SpaCy\n",
        "dt[36]= tp.named_entities_data(list(dt[6]), 'spacy')\n",
        "dt[37]= tp.named_entities_data(list(dt[7]), 'spacy')\n",
        "\n",
        "# --- With Lemmas\n",
        "# NLTK\n",
        "dt[38]= tp.named_entities_data(list(dt[8]), 'nltk')\n",
        "dt[39]= tp.named_entities_data(list(dt[9]), 'nltk')\n",
        "\n",
        "dt[40]= tp.named_entities_data(list(dt[8]), 'spacy')\n",
        "dt[41]= tp.named_entities_data(list(dt[9]), 'spacy')\n",
        "# SpaCy\n",
        "dt[42]= tp.named_entities_data(list(dt[10]), 'nltk')\n",
        "dt[43]= tp.named_entities_data(list(dt[11]), 'nltk')\n",
        "\n",
        "dt[44]= tp.named_entities_data(list(dt[10]), 'spacy')\n",
        "dt[45]= tp.named_entities_data(list(dt[11]), 'spacy')\n",
        "# Cleaned data\n",
        "# NLTK\n",
        "dt[46]= tp.named_entities_data(list(dt[12]), 'nltk')\n",
        "dt[47]= tp.named_entities_data(list(dt[13]), 'nltk')\n",
        "\n",
        "dt[48]= tp.named_entities_data(list(dt[12]), 'spacy')\n",
        "dt[49]= tp.named_entities_data(list(dt[13]), 'spacy')\n",
        "# SpaCy\n",
        "dt[50]= tp.named_entities_data(list(dt[14]), 'nltk')\n",
        "dt[51]= tp.named_entities_data(list(dt[15]), 'nltk')\n",
        "\n",
        "dt[52]= tp.named_entities_data(list(dt[14]), 'spacy')\n",
        "dt[53]= tp.named_entities_data(list(dt[15]), 'spacy')\n",
        "\n",
        "# -- Metrics computation\n",
        "pairs = {'nltk_token':[2,3], 'spacy_token':[4,5], 'clean_token':[6,7], # Tokens\n",
        "         'nltk_lemma':[8,9], 'spacy_lemma':[10,11], 'clean_nltk_lemma':[12,13], 'clean_spacy_lemma':[14,15], # Lemmas\n",
        "         'nltk_token_wsd':[16,17], 'spacy_token_wsd':[18,19], 'clean_token_wsd':[20,21], # WSD\n",
        "         'nltk_lemma_wsd':[22,23], 'spacy_lemma_wsd':[24,25], 'clean_nltk_lemma_wsd':[26,27], 'clean_spacy_lemma_wsd':[28,29], #WSD\n",
        "         'nltk_ne':[30,31],  'spacy_ne':[32,33],  'clean_nltk_ne':[34,35],  'clean_spacy_ne':[36,37], #Named Entities\n",
        "         'nltk_lemmas_-_nltk_ne':[38,39],  'nltk_lemmas_-_spacy_ne':[40,41],  'spacy_lemmas_-_nltk_ne':[42,43],  'spacy_lemmas_-_spacy_ne':[44,45], #Named Entities\n",
        "         'clean_nltk_lemmas_-_nltk_ne':[46,47],  'clean_nltk_lemmas_-_spacy_ne':[48,49],  'clean_spacy_lemmas_-_nltk_ne':[50,51],  'clean_spacy_lemmas_-_spacy_ne':[52,53] #Named Entities\n",
        "        }\n",
        "\n",
        "metrics = ['jaccard', 'cosine', 'overlap', 'dice']\n",
        "mets_results = {k:{} for k in metrics}\n",
        "mets_results['gs'] = dt['gs']\n",
        "\n",
        "for name, values in pairs.items():\n",
        "    met_results = ComputeMetrics(dt[values].to_numpy(), metrics).do()\n",
        "    for metric in metrics:\n",
        "        mets_results[metric][name] = met_results[metric]\n",
        "\n",
        "sr = ShowResults(mets_results, {'Tokenization':'token', 'Lemmatization':'lemma', 'Word Sense Disambiguation':'wsd', 'Named Entities':'ne'}, False)\n",
        "sr.dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      4.500\n",
              "1      5.000\n",
              "2      4.250\n",
              "3      4.500\n",
              "4      5.000\n",
              "       ...  \n",
              "454    5.000\n",
              "455    4.750\n",
              "456    5.000\n",
              "457    4.000\n",
              "458    3.833\n",
              "Name: gs, Length: 459, dtype: float64"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mets_results['gs'] = dt['gs']\n",
        "mets_results['gs']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Tokenization\n",
            "+-------------+---------+--------+---------+-------+\n",
            "|   Category  | jaccard | cosine | overlap |  dice |\n",
            "+-------------+---------+--------+---------+-------+\n",
            "|  NLTK TOKEN |   0.45  | 0.461  |  0.444  | 0.462 |\n",
            "| SPACY TOKEN |  0.461  | 0.472  |  0.458  | 0.473 |\n",
            "| CLEAN TOKEN |  0.468  | 0.473  |  0.443  | 0.475 |\n",
            "+-------------+---------+--------+---------+-------+\n",
            "\n",
            "--- Results for Lemmatization\n",
            "+-------------------+---------+--------+---------+-------+\n",
            "|      Category     | jaccard | cosine | overlap |  dice |\n",
            "+-------------------+---------+--------+---------+-------+\n",
            "|     NLTK LEMMA    |  0.449  | 0.462  |   0.44  | 0.464 |\n",
            "|    SPACY LEMMA    |  0.477  | 0.492  |   0.47  | 0.494 |\n",
            "|  CLEAN NLTK LEMMA |  0.481  | 0.496  |  0.465  | 0.498 |\n",
            "| CLEAN SPACY LEMMA |  0.491  | 0.505  |  0.476  | 0.507 |\n",
            "+-------------------+---------+--------+---------+-------+\n",
            "\n",
            "--- Results for Word Sense Disambiguation\n",
            "+-----------------------+---------+--------+---------+-------+\n",
            "|        Category       | jaccard | cosine | overlap |  dice |\n",
            "+-----------------------+---------+--------+---------+-------+\n",
            "|     NLTK TOKEN WSD    |   0.42  | 0.417  |  0.398  | 0.419 |\n",
            "|    SPACY TOKEN WSD    |  0.429  | 0.425  |  0.408  | 0.426 |\n",
            "|    CLEAN TOKEN WSD    |   0.48  | 0.479  |  0.449  | 0.481 |\n",
            "|     NLTK LEMMA WSD    |  0.417  | 0.408  |  0.388  |  0.41 |\n",
            "|    SPACY LEMMA WSD    |  0.422  | 0.409  |  0.388  | 0.411 |\n",
            "|  CLEAN NLTK LEMMA WSD |  0.482  | 0.483  |  0.455  | 0.485 |\n",
            "| CLEAN SPACY LEMMA WSD |  0.493  | 0.492  |  0.466  | 0.494 |\n",
            "+-----------------------+---------+--------+---------+-------+\n",
            "\n",
            "--- Results for Named Entities\n",
            "+-------------------------------+---------+--------+---------+-------+\n",
            "|            Category           | jaccard | cosine | overlap |  dice |\n",
            "+-------------------------------+---------+--------+---------+-------+\n",
            "|            NLTK NE            |   0.44  | 0.445  |   0.43  | 0.446 |\n",
            "|            SPACY NE           |  0.427  | 0.431  |  0.427  | 0.431 |\n",
            "|         CLEAN NLTK NE         |  0.468  | 0.473  |  0.443  | 0.475 |\n",
            "|         CLEAN SPACY NE        |  0.456  | 0.458  |  0.422  |  0.46 |\n",
            "|     NLTK LEMMAS - NLTK NE     |  0.438  | 0.444  |  0.424  | 0.445 |\n",
            "|     NLTK LEMMAS - SPACY NE    |   0.42  | 0.426  |  0.418  | 0.426 |\n",
            "|     SPACY LEMMAS - NLTK NE    |  0.468  | 0.484  |  0.466  | 0.485 |\n",
            "|    SPACY LEMMAS - SPACY NE    |  0.441  | 0.446  |  0.435  | 0.446 |\n",
            "|  CLEAN NLTK LEMMAS - NLTK NE  |  0.481  | 0.496  |  0.465  | 0.498 |\n",
            "|  CLEAN NLTK LEMMAS - SPACY NE |  0.468  | 0.477  |  0.442  |  0.48 |\n",
            "|  CLEAN SPACY LEMMAS - NLTK NE |  0.491  | 0.505  |  0.476  | 0.507 |\n",
            "| CLEAN SPACY LEMMAS - SPACY NE |  0.476  | 0.485  |  0.451  | 0.487 |\n",
            "+-------------------------------+---------+--------+---------+-------+\n"
          ]
        }
      ],
      "source": [
        "sr = ShowResults(mets_results, {'Tokenization':'token', 'Lemmatization':'lemma', 'Word Sense Disambiguation':'wsd', 'Named Entities':'ne'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjCFQFyeeO28"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "Until now we have tried to compute similarity of the STS task applying different levels of text preprocessing chaining different process as the preprocessing becomes more complex:\n",
        "\n",
        "1.   Tokenization\\\n",
        "    a. NLTK tokenizer\\\n",
        "    b. spaCy tokenizer\\\n",
        "    c. Manual cleaning process\n",
        "2.  Lemmatization\\\n",
        "    a. NLTK tokenizer + NLTK lemmatizer\\\n",
        "    b. spaCy tokenizer + spaCy lemmatizer\\\n",
        "    c. Manual cleaning + NLTK lemmatizer\\\n",
        "    d. Manual cleaning + spaCy lemmatizer\n",
        "3. Word Sense Disambiguation (WSD)\\\n",
        "    a. NLTK tokenizer + WSD NLTK\\\n",
        "    b. spaCy tokenizer + WSD NLTK\\\n",
        "    c. Manual Cleaning + WSD NLTK\\\n",
        "    d. NLTK tokenizer + NLTK lemmatizer + WSD NLTK\\\n",
        "    e.spaCy tokenizer + spaCy lemmatizer + WSD NLTK\\\n",
        "    f. Manual cleaning + NLTK lemmatizer + WSD NLTK\\\n",
        "    g. Manual cleaning + spaCy lemmatizer + WSD NLTK\n",
        "4. Word Sequences - Named Entities (NE) < ----------------------------------- THIS LAB SESION\\\n",
        "    a. NLTK tokenizer + NE NLTK\\\n",
        "    b. spaCy tokenizer + NE spaCy\\\n",
        "    c. Manual cleaning  + NE NLTK\\\n",
        "    d. Manual cleaning + Ne spaCy\\\n",
        "    e. NLTK tokenizer + NLTK lemmatizer + NE NLTK\\\n",
        "    f. NLTK tokenizer + NLTK lemmatizer + NE spaCy\\\n",
        "    g. spaCy tokenizer + spaCy lemmatizer + NE NLTK\\\n",
        "    h. spaCy tokenizer + spaCy lemmatizer + NE spaCy\\\n",
        "    i. Manual cleaning + NLTK lemmatizer + NE NLTK\\\n",
        "    j. Manual cleaning + NLTK lemmatizer + NE spaCy\\\n",
        "    k. Manual cleaning + spaCy lemmatizer + NE NLTK\\\n",
        "    l. Manual cleaning + spaCy lemmatizer + NE spaCy\\\n",
        "\n",
        "\n",
        "\n",
        "In this last excercise we extracted the named entities (with NLTK and spaCy implementations) and joined them as a single entity within the sentences. We applied these to the resulting sentences of previous processing techniques.\n",
        "\n",
        "Furthermore, we enhanced our Word Sense Disambiguation (WSD) implementation from the preceding lab. We introduced the capability to decide whether to retain words for which the WSD technique is inapplicable. Additionally, we incorporated the option to select between directly comparing the synset objects or solely the word substrings within the synset. (Thus, better results were obtained in the similarity measures when applying this technique.)\n",
        "\n",
        "\n",
        "It appears that using the \"CLEAN\" approach, which includes lowercasing, removing stopwords, punctuation, and short words, led to the highest STS values in general. This suggests that effective text cleaning and preprocessing can improve semantic similarity measurements.\n",
        "\n",
        "\n",
        "Generally, the use od lemmatization improved results compared to tokenization alone. Throughout most of the sessions, the approach using SpaCy's lemmatization achieved the highest similarity values, which suggests that SpaCy's lemmatization method may be more effective for the data .\n",
        "\n",
        "\n",
        "The results of applying WSD, after the last modification, seem to be less consistent in terms of improvement, with some cases performing worse. \"CLEAN SPACY LEMMA\" with manual cleaning, SpaCy's lemmatization and word sense disambiguation showed the highest values, indicating that combining cleaning, lemmatization with WSD can be effective for improving STS.\n",
        "\n",
        "\n",
        "In this lab session applying the Word Sequences technique, the results  show that its impact can be mixed, with some combinations performing better than others. Again, using manual cleaning + spaCy lemmas produced the highest STS values in the named entities category. However, in all cases, the NLTK's named entity extraction and merging process obtained higher values.\n",
        "It's worth noting that in some cases, the word sequences technique, may enhance similarity measurements, while in others, it might not provide significant benefits.\n",
        "\n",
        "According to the increase in the value of the Pearson correlation as the techniques are chained we can verify that effective text preprocessing is crucial for improving semantic textual similarity measurements. The overall results continue to support the application of manual cleaning + spaCy lemmas for more complex techniques. While the best NE result did not outperform the best WSD result (with the latest update), bot areh effective for improving our similarity measurement process. However, the choice of the best preprocessing pipeline may depend on the specific goal.It is concluded then, that the unification of named entities can be valuable, but its impact may vary depending on the preprocessing pipeline chosen.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "IHLT",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
