{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarioROT/IHLT-MAI/blob/main/Session5_MarioRosas_AlamLopez.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpEp-jhafvsx"
      },
      "source": [
        "# Lab session 5 (Lexical Semmantics) - IHLT\n",
        "\n",
        "**Students:**\n",
        "- Mario Rosas\n",
        "- Alam Lopez\n",
        "\n",
        "**Lab Professor:** Salvador Medina Herrera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0LXucmalIYk"
      },
      "source": [
        "## Paraphrases Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yeQ11CQB4e3o",
        "outputId": "3cbe5dbb-a490-4d85-961d-f481ad8231e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IHLT-MAI'...\n",
            "remote: Enumerating objects: 227, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 227 (delta 25), reused 12 (delta 12), pack-reused 190\u001b[K\n",
            "Receiving objects: 100% (227/227), 264.31 KiB | 2.57 MiB/s, done.\n",
            "Resolving deltas: 100% (115/115), done.\n",
            "Collecting python-crfsuite\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "%%shell\n",
        "git clone https://github.com/mariorot/IHLT-MAI.git\n",
        "cd 'IHLT-MAI'\n",
        "mv 'complementary_material' /content/\n",
        "mv scripts /content/\n",
        "\n",
        "pip install python-crfsuite"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scripts.models import StatisticalModels\n",
        "import numpy as np\n",
        "import nltk\n",
        "import pandas as pd\n",
        "nltk.download('treebank')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet_ic')\n",
        "from nltk.corpus import wordnet_ic\n",
        "brown_ic = wordnet_ic.ic('ic-brown.dat')"
      ],
      "metadata": {
        "id": "s7bRstKeg0kV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c86427c-4b85-40a2-b18d-d66327e6b128"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HHcaJhckB_Q"
      },
      "source": [
        "## TODO\n",
        "\n",
        " **Given the following (lemma, category) pairs:**   \\\n",
        "\n",
        "```\n",
        "('the','DT'),('man','NN'),('swim','VB'),('with','PR'),('a','DT'),('girl','NN'),('and','CC'),\n",
        "('a','DT'),('boy','NN'),('whilst','PR'),('the','DT'),('woman','NN'),('walk','VB')\n",
        "```\n",
        "\n",
        "1. For each pair, when possible, print their most frequent WordNet synset\n",
        "\n",
        "2. For each pair of words, when possible, print their corresponding least common subsumer (LCS) and their similarity value, using the following functions:\n",
        "\n",
        "  - Path Similarity\n",
        "  - Leacock-Chodorow Similarity\n",
        "  - Wu-Palmer Similarity\n",
        "  - Lin Similarity\n",
        "\n",
        "3. Normalize similarity values when necessary. What similarity seems better?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code for the models is at: https://github.com/MarioROT/IHLT-MAI/blob/main/scripts/models.py\n"
      ],
      "metadata": {
        "id": "1Zbp3bhp5gWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Lemmas= [('the','DT'),('man','NN'),('swim','VB'),('with','PR'),('a','DT'),('girl','NN'),('and','CC'),\n",
        "('a','DT'),('boy','NN'),('whilst','PR'),('the','DT'),('woman','NN'),('walk','VB')]"
      ],
      "metadata": {
        "id": "agTgAI63q2yy"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We apply a cleaning process to remove some possible duplicate values\n",
        "Clean_pairs = list(set(Lemmas))\n",
        "print(len(Lemmas)-len(Clean_pairs), 'lemma-category values are duplicates')"
      ],
      "metadata": {
        "id": "DBdgqPnsM5QO",
        "outputId": "868c8e2e-1180-4c3e-c436-33710a33147c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 lemma-category values are duplicates\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ex. 1 With the corresponding pairs cleaned and with the correct category id\n",
        "# we can now review the most frequen synset\n",
        "\n",
        "d = {'NN': 'n', 'NNS': 'n',\n",
        "       'JJ': 'a', 'JJR': 'a', 'JJS': 'a',\n",
        "       'VB': 'v', 'VBD': 'v', 'VBG': 'v', 'VBN': 'v', 'VBP': 'v', 'VBZ': 'v',\n",
        "       'RB': 'r', 'RBR': 'r', 'RBS': 'r'}\n",
        "\n",
        "def most_frequent (Lemmas):\n",
        "  results={'accepted':[], 'empty':[]}\n",
        "  for pair in range(len(Lemmas)):\n",
        "\n",
        "    if Lemmas[pair][1] in d.keys():\n",
        "\n",
        "      if len(wn.synsets(Lemmas[pair][0],d[Lemmas[pair][1]]))==0:\n",
        "         results['empty'].append(Lemmas[pair])\n",
        "      else:\n",
        "        common= wn.synsets(Lemmas[pair][0],d[Lemmas[pair][1]])[0]\n",
        "        tupple=(Lemmas[pair][0],common)\n",
        "        results['accepted'].append(tupple)\n",
        "\n",
        "    else:\n",
        "      results['empty'].append(Lemmas[pair])\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "A0bTvoDzLGIZ"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Lemmas_syn=most_frequent(Clean_pairs)\n",
        "print(Lemmas_syn)"
      ],
      "metadata": {
        "id": "iVBJT_GBhVk-",
        "outputId": "246b4e9e-0c67-4e86-ec38-3ef0e652c424",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accepted': [('swim', Synset('swim.v.01')), ('walk', Synset('walk.v.01')), ('man', Synset('man.n.01')), ('girl', Synset('girl.n.01')), ('woman', Synset('woman.n.01')), ('boy', Synset('male_child.n.01'))], 'empty': [('with', 'PR'), ('and', 'CC'), ('whilst', 'PR'), ('a', 'DT'), ('the', 'DT')]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "07VPNSVeU8AU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now, we put all the lemma-category in all the possible pair combinations into the list \"Pairs\"\n",
        "#Since there are 11 pairs, we should get 55 elements\n",
        "Pairs = [(a, b) for idx, a in enumerate(Lemmas_syn['accepted']) for b in Lemmas_syn['accepted'][idx + 1:]]\n",
        "Pairs"
      ],
      "metadata": {
        "id": "4n1he-eiMqQW",
        "outputId": "06711067-bce7-48de-bcdc-1933a75ffb74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('swim', Synset('swim.v.01')), ('walk', Synset('walk.v.01'))),\n",
              " (('swim', Synset('swim.v.01')), ('man', Synset('man.n.01'))),\n",
              " (('swim', Synset('swim.v.01')), ('girl', Synset('girl.n.01'))),\n",
              " (('swim', Synset('swim.v.01')), ('woman', Synset('woman.n.01'))),\n",
              " (('swim', Synset('swim.v.01')), ('boy', Synset('male_child.n.01'))),\n",
              " (('walk', Synset('walk.v.01')), ('man', Synset('man.n.01'))),\n",
              " (('walk', Synset('walk.v.01')), ('girl', Synset('girl.n.01'))),\n",
              " (('walk', Synset('walk.v.01')), ('woman', Synset('woman.n.01'))),\n",
              " (('walk', Synset('walk.v.01')), ('boy', Synset('male_child.n.01'))),\n",
              " (('man', Synset('man.n.01')), ('girl', Synset('girl.n.01'))),\n",
              " (('man', Synset('man.n.01')), ('woman', Synset('woman.n.01'))),\n",
              " (('man', Synset('man.n.01')), ('boy', Synset('male_child.n.01'))),\n",
              " (('girl', Synset('girl.n.01')), ('woman', Synset('woman.n.01'))),\n",
              " (('girl', Synset('girl.n.01')), ('boy', Synset('male_child.n.01'))),\n",
              " (('woman', Synset('woman.n.01')), ('boy', Synset('male_child.n.01')))]"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for pair in Pairs:\n",
        "  pair[0][1].lowest_common_hypernyms(pair[1][1])\n",
        "  pair[0][1].path_similarity(pair[1][1])\n",
        "  pair[0][1].lowest_common_hypernyms(pair[1][1])\n",
        "  print(test)"
      ],
      "metadata": {
        "id": "pfsD6uStsobl",
        "outputId": "be343c59-bdda-48f6-f47e-75d79fd21c9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('travel.v.01')]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[Synset('adult.n.01')]\n",
            "[Synset('adult.n.01')]\n",
            "[Synset('male.n.02')]\n",
            "[Synset('woman.n.01')]\n",
            "[Synset('person.n.01')]\n",
            "[Synset('person.n.01')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Similarities example\n",
        "\n",
        "#def similar (pairs):\n",
        " # for pair in pairs:\n",
        "  #  pairs\n",
        "#wn.synset('dog.n.01')\n",
        "\n",
        "wn.synset('cat.n.01')\n",
        "\n",
        "#wn.synset('dog.n.01').lowest_common_hypernyms( wn.synset('cat.n.01'))\n",
        "\n",
        "#wn.synset('dog.n.01').lowest_common_hypernyms( Synset('cat.n.01'))\n",
        "#  [Synset('carnivore.n.01')]\n",
        "#dog.path_similarity(cat)  👉  0.2\n",
        "#dog.lch_similarity(cat)  👉  2.0281482472922856\n",
        "#dog.wup_similarity(cat)  👉  0.8571428571428571\n",
        "#dog.lin_similarity(cat,brown_ic)  👉  0.8768009843733973"
      ],
      "metadata": {
        "id": "WRp0pJTLmZLw",
        "outputId": "f407a613-9316-4559-dcec-8424896ee38b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('cat.n.01')"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synsets(Lemma[pair][0],d[Lemma[pair][1]])[0]"
      ],
      "metadata": {
        "id": "lLBzM4R_ZKF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ".lowest_common_hypernyms('cat.n.01')"
      ],
      "metadata": {
        "id": "PEPt-iJevwuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjCFQFyeeO28"
      },
      "source": [
        "# Conclusion\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "IHLT",
      "language": "python",
      "name": "ihlt"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "0795eca24a98e58b2dcbec80c9554a91f94c5c7d4e675f06c8c2f85c434623a5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}