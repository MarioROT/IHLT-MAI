{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarioROT/IHLT-MAI/blob/main/Session5_MarioRosas_AlamLopez.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpEp-jhafvsx"
      },
      "source": [
        "# Lab session 5 (Lexical Semmantics) - IHLT\n",
        "\n",
        "**Students:**\n",
        "- Mario Rosas\n",
        "- Alam Lopez\n",
        "\n",
        "**Lab Professor:** Salvador Medina Herrera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0LXucmalIYk"
      },
      "source": [
        "## Paraphrases Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yeQ11CQB4e3o",
        "outputId": "e6497594-9a37-47e5-f8b5-dc639de385df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IHLT-MAI'...\n",
            "remote: Enumerating objects: 224, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 224 (delta 23), reused 12 (delta 12), pack-reused 190\u001b[K\n",
            "Receiving objects: 100% (224/224), 261.63 KiB | 17.44 MiB/s, done.\n",
            "Resolving deltas: 100% (113/113), done.\n",
            "Collecting python-crfsuite\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "%%shell\n",
        "git clone https://github.com/mariorot/IHLT-MAI.git\n",
        "cd 'IHLT-MAI'\n",
        "mv 'complementary_material' /content/\n",
        "mv scripts /content/\n",
        "\n",
        "pip install python-crfsuite"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scripts.models import StatisticalModels\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('treebank')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "s7bRstKeg0kV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd565857-26a2-4935-851a-6e8eb3d0470f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HHcaJhckB_Q"
      },
      "source": [
        "## TODO\n",
        "\n",
        " **Given the following (lemma, category) pairs:**   \\\n",
        "\n",
        " (’the’,’DT’), (’man’,’NN’), (’swim’,’VB’), (’with’, ’PR’), (’a’, ’DT’),\n",
        "(’girl’,’NN’), (’and’, ’CC’), (’a’, ’DT’), (’boy’, ’NN’), (’whilst’, ’PR’),\n",
        "(’the’, ’DT’), (’woman’, ’NN’), (’walk’, ’VB’)\n",
        "\n",
        "1. For each pair, when possible, print their most frequent WordNet synset\n",
        "\n",
        "2. For each pair of words, when possible, print their corresponding least common subsumer (LCS) and their similarity value, using the following functions:\n",
        "\n",
        "  - Path Similarity\n",
        "  - Leacock-Chodorow Similarity\n",
        "  - Wu-Palmer Similarity\n",
        "  - Lin Similarity\n",
        "\n",
        "3. Normalize similarity values when necessary. What similarity seems better?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code for the models is at: https://github.com/MarioROT/IHLT-MAI/blob/main/scripts/models.py\n"
      ],
      "metadata": {
        "id": "1Zbp3bhp5gWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Lemmas= [('the','DT'), ('man','NN'), ('swim','VB'), ('with', 'PR'), ('a', 'DT'),\n",
        "('girl','NN'), ('and', 'CC'), ('a', 'DT'), ('boy', 'NN'), ('whilst', 'PR'),\n",
        "('the', 'DT'), ('woman', 'NN'), ('walk', 'VB')]"
      ],
      "metadata": {
        "id": "agTgAI63q2yy"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We apply a cleaning process to remove some possible duplicate values\n",
        "\n",
        "Clean_pairs = list(set(Lemmas))\n",
        "print(len(Lemmas)-len(Clean_pairs), 'lemma-category values are duplicates')"
      ],
      "metadata": {
        "id": "DBdgqPnsM5QO",
        "outputId": "b4e3fed3-6036-487c-c296-e3b2414bcdc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 lemma-category values are duplicates\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Then we translate the category for the ones that are accepted\n",
        "wnl = nltk.stem.WordNetLemmatizer()\n",
        "def lemmatize(p):\n",
        "  d = {'NN': 'n', 'NNS': 'n',\n",
        "       'JJ': 'a', 'JJR': 'a', 'JJS': 'a',\n",
        "       'VB': 'v', 'VBD': 'v', 'VBG': 'v', 'VBN': 'v', 'VBP': 'v', 'VBZ': 'v',\n",
        "       'RB': 'r', 'RBR': 'r', 'RBS': 'r'}\n",
        "  if p[1] in d:\n",
        "    return wnl.lemmatize(p[0], pos=d[p[1]])\n",
        "  return p[0]\n",
        "\n",
        "#Clean_pairs =[ (a, b) for pair, a in Clean_pairs[a]   lemmatize(pair) for pair in Clean_pairs[pair][1]]\n",
        "\n",
        "#Pairs = [(a, b) for idx, a in enumerate(Lemmas) for b in Lemmas[idx + 1:]]\n",
        "print(lemmatize())"
      ],
      "metadata": {
        "id": "H7R63Sx2iJsm",
        "outputId": "58642aff-1840-4bdf-ea03-029563f3ee36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('women', 'NNS')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ex. 1 With the corresponding pairs cleaned and with the correct category id\n",
        "# we can now review the most frequen synset\n",
        "# First by using the synset function and then calculating by counting the words in the corpus\n",
        "\n",
        "def most_frequent (Lemma):\n",
        "  Most_common=[]\n",
        "  for pair in Lemma:\n",
        "    word=wn.synsets(Lemma)[0]\n",
        "    Most_common.append(word)\n",
        "  return Most_common"
      ],
      "metadata": {
        "id": "A0bTvoDzLGIZ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_frequent(Lemmas)"
      ],
      "metadata": {
        "id": "CGETd0KtMt7j",
        "outputId": "2fb54486-8628-427e-def2-8520b38487d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-ca8f4a10669e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmost_frequent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLemmas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-44-8791d7561023>\u001b[0m in \u001b[0;36mmost_frequent\u001b[0;34m(Lemma)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mMost_common\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mLemma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLemma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mMost_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mMost_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36msynsets\u001b[0;34m(self, lemma, pos, lang, check_exceptions)\u001b[0m\n\u001b[1;32m   1748\u001b[0m         \u001b[0mof\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \"\"\"\n\u001b[0;32m-> 1750\u001b[0;31m         \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#First, we put all the lemma-category in all the possible pair combinations into the list \"Pairs\"\n",
        "#Since there are 13 pairs, we should get 78 elements\n",
        "Pairs = [(a, b) for idx, a in enumerate(Lemmas) for b in Lemmas[idx + 1:]]\n",
        "len(Pairs)"
      ],
      "metadata": {
        "id": "4n1he-eiMqQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sm = StatisticalModels(data, [500,1000,1500,2000,2500,3000], 3000, ['HMM', 'TnT', 'PER', 'CRF'])\n"
      ],
      "metadata": {
        "id": "pLYa-qixlJkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "var1=sm.do()"
      ],
      "metadata": {
        "id": "WRp0pJTLmZLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sm.results()"
      ],
      "metadata": {
        "id": "PEPt-iJevwuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjCFQFyeeO28"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "**Which model would you select? Justify the answer.**\n",
        "\n",
        "1. From the previous graph and tables, we can get some insights:\n",
        "\n",
        "  - PER was the model with the best accuracy in all the cases, from 500 to 3000 sentences.\n",
        "\n",
        "  - HMM was the model with the shortest average time for training while the CRF model was the one with the longest.\n",
        "\n",
        "  - CRF was the model with the shortest average time for testing, followed by PER with the double of time than CRF.\n",
        "\n",
        "  - HMM was the model with the shortest average overall (training+testing).\n",
        "  \n",
        "  - TnT and HMM models are the most sensitive to the amount of data in the training. When we increased from 500 to 3000 sentences, they improved the accuracy by 10 pp, while the PER and the CRF improved only by 5 pp. So, the size of the training data is relevant.\n",
        "\n",
        "\n",
        "2. Since there is no model that outperfomed the others in all the parameters (accuracy, training time, testing time, overall time), the definition of the best model and therefore the model that we would select, depends on the specific needs of the problem to solve. We need to consider the amount of data for training and the time constraint.\n",
        "\n",
        "  - If we are focusing, specifically on accuracy, and the time of training or testing is not a constraint, then the PER would be the best option.\n",
        "\n",
        "  - If we are pursuing to get a near real time model to test some data, and the time for training is not a constraint then the CRF will be the best option.\n",
        "  - If our goal is to have a model that training time is the main objective because we are going to train it very often, the HMM is the optimal.\n",
        "  - Finally if the main focus is the overall time of the model due to some limited resources then again the HMM is the best option.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "IHLT",
      "language": "python",
      "name": "ihlt"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "0795eca24a98e58b2dcbec80c9554a91f94c5c7d4e675f06c8c2f85c434623a5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}